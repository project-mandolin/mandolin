## A simple example training multi-layer perceptron using MXNET on a portion of the mnist data
mandolin {
    data     = ${?PWD}
    mode     = "train"

    trainer {
    	    train-file = ${mandolin.data}/mnist.10k
	    test-file  = ${mandolin.data}/mnist.t
	    label-file = ${mandolin.data}/mnist.labels
	    dense-vector-size = 784
	    model-file = ${mandolin.data}/mnist.model
	    num-epochs = 20
	    mini-batch-size = 128
	    
	    scale-inputs = true	    
            specification = [{"ltype": "Input"}, {"ltype": "SoftMax"}]
    }

    mx {
       num-classes = 10
       input-type  = mandolin
       gpus = []
       cpus = [0]
       save-freq = 20
       train {
       	     optimizer = "sgd"
	     initial-learning-rate = 0.1
	     wd = 0.0000001
	     initializer = "xavier"
       }
       ## Basic MLP
       specification {
         fc0 =  {"data": "input", "type": "fc", "num_hidden": 200},
	 act0 = {"data": "fc0", "type":"activation", "act_type": "relu"},
	 fc1 =  {"data": "act0", "type": "fc", "num_hidden": 200},
	 act1 = {"data": "fc1", "type":"activation", "act_type": "relu"},
	 fc2 =  {"data": "act1", "type": "fc", "num_hidden": 200},
	 act2 = {"data": "fc2", "type":"activation", "act_type": "relu"},
	 fcFinal =  {"data": "act2", "type": "fc", "num_hidden": 10},
	 soft1 = {"data": "fcFinal", "type": "softmax"}
       }
    }
    

}