## A simple example training multi-layer perceptron using MXNET on a portion of the mnist data
mandolin {
    data     = ${?PWD}
    mode     = "train"
    driver   = "mx"

    mx {
        train-file = ${mandolin.data}/mnist.10k
        test-file  = ${mandolin.data}/mnist.t
	label-file = ${mandolin.data}/mnist.labels
	eval-freq  = 10
	dense-vector-size = 784
	model-file = ${mandolin.data}/mnist.model
	progress-file = ${mandolin.data}/mnist.progress
	num-epochs = 20
	mini-batch-size = 128	    
	scale-inputs = true	    

       num-classes = 10
       input-type  = mandolin
       gpus = []
       cpus = [0]
       save-freq = 10

       train {
       	     optimizer = "sgd"
	     initial-learning-rate = 0.1
	     wd = 0.0000001
	     initializer = "xavier"
       }

       ## Basic MLP
       specification = [
       	 {"type": "fc", "name": "fc0", "data": "input", "num_hidden": 200},
	 {"type": "activation", "name": "act0", "data": "fc0", "act_type": "relu"},
       	 {"type": "fc", "name": "fc1", "data": "act0", "num_hidden": 200},
	 {"type": "activation", "name": "act1", "data": "fc1", "act_type": "relu"},
       	 {"type": "fc", "name": "fc2", "data": "act1", "num_hidden": 200}, 
	 {"type": "activation", "name": "act2", "data": "fc2", "act_type": "relu"},
	 {"type": "fc", "name": "fcFinal", "data": "act2", "num_hidden": 10}, ## set number of classes
	 {"type": "softmax", "name": "soft1", "data": "fcFinal"}
       ]
    }
    

}