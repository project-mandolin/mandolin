## A simple example training multi-layer perceptron using MXNET on a portion of the mnist data
mandolin {
    data     = ${?PWD}
    mode     = "train"

    trainer {
    	    train-file = ${mandolin.data}/mnist.10k
	    test-file  = ${mandolin.data}/mnist.t
	    label-file = ${mandolin.data}/mnist.labels
	    dense-vector-size = 784
	    model-file = ${mandolin.data}/mnist.model
	    num-epochs = 20
	    mini-batch-size = 128
	    
	    scale-inputs = true	    
            specification = [{"ltype": "Input"}, {"ltype": "SoftMax"}]
    }

    mx {
       num-classes = 10
       input-type  = mandolin
       gpus = []
       cpus = [0]
       save-freq = 4
       ## Basic MLP
       specification = [
       	 {"type": "fc", "name": "fc0", "data": "input", "num_hidden": 200},
	 {"type": "activation", "name": "act0", "data": "fc0", "act_type": "relu"},
       	 {"type": "fc", "name": "fc1", "data": "act0", "num_hidden": 200},
	 {"type": "activation", "name": "act1", "data": "fc1", "act_type": "relu"},
       	 {"type": "fc", "name": "fc2", "data": "act1", "num_hidden": 200}, 
	 {"type": "activation", "name": "act2", "data": "fc2", "act_type": "relu"},
	 {"type": "fc", "name": "fcFinal", "data": "act2", "num_hidden": 10}, ## set number of classes
	 {"type": "softmax", "name": "soft1", "data": "fcFinal"}
       ]
    }
    

}