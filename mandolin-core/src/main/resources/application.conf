## This includes the configuration options for Mandolin applications
## Specific apps should override and/or extend these values

spark {
    cores.max = 10    
    storage=mem_only
    serializer=org.apache.spark.serializer.KryoSerializer
    kryoserializer.buffer.mb=640
    executor.memory=24g
    app.name = "Mandolin"
}

mandolin {

    name		= "Mandolin"
    driver		= "org.mitre.mandolin.glp.local.MandolinWithoutSpark"
    mode		= train-test ## train-test|train|decode|train-decode
    partitions          = 10
    
    trainer {
        train-file	= null
        test-file	= null
        progress-file   = null ## progress of training: training loss and test accuracy
        eval-freq	= 1
        test-partitions	= 0 ## set this to a positive value to specify spark partitions to use for decoding/testing
        model-file	= null
        num-hash-features = 100000 ## size of hashed feature space, if random features used
        use-random-features = false ## setting to true will force use of hashed/random features

        ensure-sparse	= true ## ensure sparse handling of weights; set to false if weights are dense-ish
        dense-vector-size = 0 ## set this positive to provide a fixed dense vector size input for efficiency in processing dense data

        ## oversample - changes optimization data RDD construction to oversample partitions
	## a value of 1.0 says that each partition should have n elements; where n = numPartitions/totalData
	## a value of 5.0 says each partition should have 5*n elements, etc.
	## value of 0.0 indicates no oversampling
	oversample	= 0.0
	##########

        detail-file	= null
        dense-output-file = null ## can be set to dump out vectors in dense CSV format
    
        num-epochs	= 20
        num-subepochs	= 1

        coef1		= 1.0  ## C-parameter to utilize for hinge-type loss functions
        qval		= 1.0  ## coefficient controlling translog loss curve

        print-feature-file = null 
        max-features-mi	= -1          ## select top N features based on mutual information (gain)
        scale-inputs	= false           ## unit scale inputs

        updater-compose-strategy = "minimum"
        max-norm	= false               ## use max-norm reglurization on weights
   
        threads	= 8   ## number of gradient-computing threads for SGD-like algorithms
        synchronous	= false

        skip-probability  = 0.0 ## probability of randomly skipping a training instance (fast way to subsample instances)
        mini-batch-size = 1


	specification = [
	      {"ltype":"InputSparse"},
	      {"ltype":"SoftMax"} ]
    
        density-ratio = 0.1

        optimizer {
                method	= sgd
                initial-learning-rate = 0.1
                lambda	= 0.001 # larger values will cause faster geometric decay
                epsilon	= 0.00001 # smoothing value for adadelta update
                rho	= 0.95    # decay/mix rate for historical updates/gradients
        }

    }
    
    # handle different decoder apps
    decoder {
        input-file	= null
        model-file	= null
        output-file	= null
    }
      
}
